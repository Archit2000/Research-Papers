{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_global = 0.1\n",
    "k_global = 0\n",
    "test_global = 0.1\n",
    "test_global_num = None\n",
    "val_global = 0.1\n",
    "val_global_num = None\n",
    "dataset_global = \"Cora\"\n",
    "thresh = 1e-10\n",
    "alpha_param,beta_param,gamma_param,lambda_param,delta_param = 100.0,1.0,100.0,0.01,1e-06,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from networkx.algorithms import community\n",
    "from networkx.generators.community import random_partition_graph\n",
    "from networkx.generators.community import stochastic_block_model\n",
    "from networkx.generators.random_graphs import barabasi_albert_graph\n",
    "from networkx.generators.random_graphs import erdos_renyi_graph\n",
    "from networkx.generators.random_graphs import watts_strogatz_graph\n",
    "from random import sample\n",
    "from scipy.sparse import csgraph\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import random\n",
    "from scipy.sparse import random\n",
    "from scipy.sparse.linalg import inv\n",
    "from scipy.sparse.linalg import norm\n",
    "from scipy.stats import rv_continuous\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from torch import Tensor\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse,homophily\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_networkx\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import pandas as pd\n",
    "from pyswarm import pso\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if data exists. If not downloading the data.\n",
      "Dataset Information\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "-------------||-------------\n",
      "Number of Nodes : 2708\n",
      "The data will be split into training, tesing and validation\n",
      "Ratio of Training: 0.8 No of Nodes = 2168\n",
      "Ratio of Testing: 0.1 , No of edges = 270\n",
      "Ratio of Validation: 0.1 , No of Nodes = 270\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(\"Checking if data exists. If not downloading the data.\")\n",
    "dataset = Planetoid(root='data/planetoid', name=dataset_global)\n",
    "print(\"Dataset Information\")\n",
    "print(dataset[0])\n",
    "dataset = dataset[0]\n",
    "print(\"-------------||-------------\")\n",
    "print(\"Number of Nodes :\",dataset.x.shape[0])\n",
    "print(\"The data will be split into training, tesing and validation\")\n",
    "test_global_num = int(dataset.x.shape[0]*(test_global))\n",
    "val_global_num = int(dataset.x.shape[0]*(val_global))\n",
    "print(\"Ratio of Training:\",(1-test_global-val_global),\"No of Nodes =\",dataset.x.shape[0]-test_global_num-val_global_num)\n",
    "print(\"Ratio of Testing:\",(test_global),\", No of edges =\",test_global_num)\n",
    "print(\"Ratio of Validation:\",(val_global),\", No of Nodes =\",val_global_num)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Restructuring the database into testing, traing, validation\n",
      "Dataset Information:\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Number of Nodes : 2708\n",
      "Number of classes: 7\n",
      "\n",
      "The data will be split into training, tesing and validation\n",
      "Ratio of Training: 0.8 No of Nodes = 2168\n",
      "Ratio of Testing: 0.1 , No of edges = 270\n",
      "Ratio of Validation: 0.1 , No of Nodes = 270\n",
      "\n",
      "Homophilic ratio : 0.8099659085273743\n",
      "Sparsity of Graph: 0.0028799998253884154\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Restructuring the database into testing, traing, validation\")\n",
    "dataset = Planetoid(root='data/planetoid', split=\"random\",num_val= val_global_num, num_test= test_global_num, name=dataset_global)\n",
    "dataset = dataset[0]\n",
    "print(\"Dataset Information:\")\n",
    "print(dataset)\n",
    "print(\"Number of Nodes :\",dataset.x.shape[0])\n",
    "NO_OF_CLASSES = len(set(dataset.y.numpy()))\n",
    "print(\"Number of classes:\",NO_OF_CLASSES)\n",
    "print()\n",
    "print(\"The data will be split into training, tesing and validation\")\n",
    "test_global_num = int(dataset.x.shape[0]*(test_global))\n",
    "val_global_num = int(dataset.x.shape[0]*(val_global))\n",
    "train_global_num = dataset.x.shape[0]-test_global_num-val_global_num\n",
    "print(\"Ratio of Training:\",(1-test_global-val_global),\"No of Nodes =\",dataset.x.shape[0]-test_global_num-val_global_num)\n",
    "print(\"Ratio of Testing:\",(test_global),\", No of edges =\",test_global_num)\n",
    "print(\"Ratio of Validation:\",(val_global),\", No of Nodes =\",val_global_num)\n",
    "print()\n",
    "print(\"Homophilic ratio : \" + str(homophily(dataset.edge_index,dataset.y,method='edge')))\n",
    "NO_OF_EDGES = dataset.edge_index.shape[1]\n",
    "N = dataset.x.shape[0]\n",
    "sparsity = 2*NO_OF_EDGES/(N*(N-1))\n",
    "print(\"Sparsity of Graph:\",sparsity)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Generate Adjecency Matrix\n",
      "Adjacency Matrix: torch.Size([2708, 2708])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "\n",
      "Convert Adjacency to Laplacian Matrix\n",
      "\n",
      "Laplacian Matrix: torch.Size([2708, 2708])\n",
      "tensor([[ 3.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  3., -1.,  ...,  0.,  0.,  0.],\n",
      "        [ 0., -1.,  5.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  4., -1.],\n",
      "        [ 0.,  0.,  0.,  ...,  0., -1.,  4.]])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Generate Adjecency Matrix\")\n",
    "adj = to_dense_adj(dataset.edge_index)\n",
    "adj = adj[0]\n",
    "print(\"Adjacency Matrix:\", adj.shape)\n",
    "print(adj)\n",
    "print(\"\\nConvert Adjacency to Laplacian Matrix\\n\")\n",
    "def get_laplacian(adj):\n",
    "    b=torch.ones(adj.shape[0])\n",
    "    return torch.diag(adj@b)-adj\n",
    "theta = get_laplacian(adj)\n",
    "print(\"Laplacian Matrix:\", theta.shape)\n",
    "print(theta)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Splitting Adjacency Matrix into Training, Testing, Validation\n",
      "Training:\n",
      "torch.Size([2168, 2168])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Testing:\n",
      "torch.Size([270, 270])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Validation:\n",
      "torch.Size([270, 270])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Splitting Adjacency Matrix into Training, Testing, Validation\")\n",
    "print(\"Training:\")\n",
    "TRAIN_adj = adj[:dataset.x.shape[0]-test_global_num-val_global_num,:dataset.x.shape[0]-test_global_num-val_global_num]\n",
    "print(TRAIN_adj.shape)\n",
    "print(TRAIN_adj)\n",
    "\n",
    "print(\"Testing:\")\n",
    "TEST_adj = adj[train_global_num:train_global_num+test_global_num,train_global_num:train_global_num+test_global_num]\n",
    "print(TEST_adj.shape)\n",
    "print(TEST_adj)\n",
    "\n",
    "print(\"Validation:\")\n",
    "VAL_adj = adj[train_global_num+test_global_num:train_global_num+test_global_num+val_global_num,train_global_num+test_global_num:train_global_num+test_global_num+val_global_num]\n",
    "print(VAL_adj.shape)\n",
    "print(VAL_adj)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Splitting Laplacian Matrix into Training, Testing, Validation\n",
      "Training:\n",
      "torch.Size([2168, 2168])\n",
      "tensor([[ 2.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  3., -1.,  ...,  0.,  0.,  0.],\n",
      "        [ 0., -1.,  5.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  3.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  2.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  1.]])\n",
      "Testing:\n",
      "torch.Size([270, 270])\n",
      "tensor([[ 1., -1.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])\n",
      "Validation:\n",
      "torch.Size([270, 270])\n",
      "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  ...,  0., -1.,  1.]])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Splitting Laplacian Matrix into Training, Testing, Validation\")\n",
    "print(\"Training:\")\n",
    "TRAIN_theta = adj[:dataset.x.shape[0]-test_global_num-val_global_num,:dataset.x.shape[0]-test_global_num-val_global_num]\n",
    "TRAIN_theta = get_laplacian(TRAIN_theta)\n",
    "print(TRAIN_theta.shape)\n",
    "print(TRAIN_theta)\n",
    "\n",
    "print(\"Testing:\")\n",
    "TEST_theta = adj[train_global_num:train_global_num+test_global_num,train_global_num:train_global_num+test_global_num]\n",
    "TEST_theta = get_laplacian(TEST_theta)\n",
    "print(TEST_theta.shape)\n",
    "print(TEST_theta)\n",
    "\n",
    "print(\"Validation:\")\n",
    "VAL_theta = adj[train_global_num+test_global_num:train_global_num+test_global_num+val_global_num,train_global_num+test_global_num:train_global_num+test_global_num+val_global_num]\n",
    "VAL_theta = get_laplacian(VAL_theta)\n",
    "print(VAL_theta.shape)\n",
    "print(VAL_theta)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Generating Edge Indexes for Traing, Testing and Validation\n",
      "tensor([[   0,    0,    1,  ..., 2166, 2166, 2167],\n",
      "        [ 633, 1862,    2,  ...,  118, 1029,  957]])\n",
      "torch.Size([2, 7754])\n",
      "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,   9,  12,  12,  13,\n",
      "          13,  13,  14,  14,  14,  14,  14,  14,  15,  18,  25,  26,  26,  27,\n",
      "          28,  29,  30,  31,  31,  31,  32,  33,  34,  35,  39,  43,  44,  46,\n",
      "          47,  48,  48,  48,  52,  53,  54,  54,  55,  56,  56,  56,  56,  57,\n",
      "          57,  58,  59,  60,  60,  60,  62,  63,  63,  64,  65,  65,  68,  69,\n",
      "          69,  70,  70,  70,  71,  72,  72,  73,  73,  74,  75,  75,  75,  76,\n",
      "          77,  77,  78,  78,  78,  79,  80,  81,  83,  84,  91,  91,  98,  98,\n",
      "          98,  99, 100, 101, 102, 102, 113, 116, 117, 117, 118, 119, 120, 123,\n",
      "         123, 123, 124, 124, 128, 129, 129, 130, 131, 136, 137, 138, 153, 159,\n",
      "         160, 161, 161, 162, 162, 163, 167, 167, 168, 169, 171, 172, 175, 176,\n",
      "         178, 181, 181, 187, 187, 188, 188, 188, 189, 189, 190, 191, 196, 199,\n",
      "         199, 199, 199, 199, 200, 200, 201, 201, 202, 202, 202, 203, 203, 204,\n",
      "         207, 214, 215, 217, 222, 223, 226, 226, 226, 226, 227, 227, 228, 229,\n",
      "         231, 231, 233, 233, 234, 235, 237, 242, 243, 245, 245, 246, 247, 248,\n",
      "         249, 257, 258, 259, 260, 263, 264, 266, 266, 267],\n",
      "        [  1,   0,   3,   2,  14,   6,   5,   8,   7, 199, 203,  14,  15,  28,\n",
      "          29,  34,   4,  12,  30,  63,  64,  65,  12,  31,  26,  25,  27,  26,\n",
      "          13,  13,  14,  18,  32,  33,  31,  31,  13, 162,  73, 188,  48,  48,\n",
      "          48,  44,  46,  47,  53,  52,  56,  57,  56,  54,  55,  57,  58,  54,\n",
      "          56,  56,  60,  59, 226, 227, 128,  14, 124,  14,  14, 123,  69,  68,\n",
      "          70,  69,  71,  72,  70,  70, 231,  39,  74,  73,  76,  77,  78,  75,\n",
      "          75,  78,  75,  77,  79,  78,  81,  80,  84,  83, 167, 169,  99, 153,\n",
      "         196,  98, 123, 102, 101, 207, 237, 117, 116, 118, 117, 120, 119,  65,\n",
      "         100, 124,  63, 123,  62, 130, 131, 129, 129, 217, 138, 137,  98, 160,\n",
      "         159, 162, 163,  35, 161, 161,  91, 168, 167,  91, 172, 171, 176, 175,\n",
      "         181, 178, 190, 188, 189,  43, 187, 189, 187, 188, 181, 204,  98,   9,\n",
      "         200, 201, 202, 203, 199, 202, 199, 202, 199, 200, 201,   9, 199, 191,\n",
      "         102, 215, 214, 136, 223, 222,  60, 227, 228, 229,  60, 226, 226, 226,\n",
      "          72, 266, 266, 267, 235, 234, 113, 243, 242, 246, 247, 245, 245, 249,\n",
      "         248, 258, 257, 260, 259, 264, 263, 231, 233, 233]])\n",
      "torch.Size([2, 206])\n",
      "tensor([[  1,   1,   2,   4,   4,   6,   7,   8,   8,   9,  10,  13,  14,  14,\n",
      "          19,  19,  20,  21,  26,  27,  29,  30,  33,  35,  38,  45,  46,  55,\n",
      "          56,  57,  65,  65,  71,  76,  77,  77,  78,  79,  80,  81,  84,  92,\n",
      "          92,  92,  92,  96,  97, 100, 101, 101, 102, 116, 117, 122, 123, 127,\n",
      "         127, 128, 129, 132, 132, 134, 135, 156, 161, 163, 164, 165, 171, 176,\n",
      "         177, 180, 181, 186, 187, 188, 192, 194, 196, 196, 197, 198, 198, 210,\n",
      "         211, 213, 214, 216, 217, 218, 227, 228, 229, 230, 232, 233, 233, 233,\n",
      "         233, 234, 235, 236, 236, 237, 237, 241, 241, 242, 243, 244, 245, 247,\n",
      "         249, 250, 251, 251, 255, 256, 257, 258, 261, 263, 268, 269],\n",
      "        [ 92, 214,  92,  92, 176,  65,  65,   9,  10,   8,   8,  14,  13, 247,\n",
      "          20, 211,  19, 251,  27,  26,  30,  29,  55, 210, 213,  46,  45,  33,\n",
      "          57,  56,   6,   7,  81,  77,  76,  92,  79,  78, 100,  71, 218,   1,\n",
      "           2,   4,  77,  97,  96,  80, 102, 194, 101, 251, 161, 123, 122, 128,\n",
      "         129, 127, 127, 134, 135, 132, 132, 242, 117, 171, 165, 164, 163,   4,\n",
      "         258, 181, 180, 263, 188, 187, 261, 101, 197, 198, 196, 196, 255,  35,\n",
      "          19,  38,   1, 217, 216,  84, 228, 227, 230, 229, 241, 234, 235, 236,\n",
      "         237, 233, 233, 233, 237, 233, 236, 232, 244, 156, 245, 241, 243,  14,\n",
      "         250, 249,  21, 116, 198, 257, 256, 177, 192, 186, 269, 268]])\n",
      "torch.Size([2, 124])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Generating Edge Indexes for Traing, Testing and Validation\")\n",
    "def adj2edge_index(adj):\n",
    "    edge_index = np.vstack(np.nonzero(adj)).T\n",
    "    return torch.from_numpy(edge_index)\n",
    "TRAIN_edge_index = adj2edge_index(TRAIN_adj)\n",
    "print(TRAIN_edge_index)\n",
    "print(TRAIN_edge_index.shape)\n",
    "\n",
    "TEST_edge_index = adj2edge_index(TEST_adj)\n",
    "print(TEST_edge_index)\n",
    "print(TEST_edge_index.shape)\n",
    "\n",
    "VAL_edge_index = adj2edge_index(VAL_adj)\n",
    "print(VAL_edge_index)\n",
    "print(VAL_edge_index.shape)\n",
    "print(\"-------------||-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Defining Feature Matrix for Training, Testing, Validation\n",
      "TRAIN_X Shape: torch.Size([2168, 1433])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "TEST_X Shape: torch.Size([270, 1433])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "VAL_X Shape: torch.Size([270, 1433])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Defining Feature Matrix for Training, Testing, Validation\")\n",
    "TRAIN_X = dataset.x[:train_global_num]\n",
    "print(\"TRAIN_X Shape:\",TRAIN_X.shape)\n",
    "print(TRAIN_X)\n",
    "TEST_X = dataset.x[train_global_num:train_global_num+test_global_num]\n",
    "print(\"TEST_X Shape:\",TEST_X.shape)\n",
    "print(TEST_X)\n",
    "VAL_X = dataset.x[train_global_num+test_global_num:train_global_num+test_global_num+val_global_num]\n",
    "print(\"VAL_X Shape:\",VAL_X.shape)\n",
    "print(VAL_X)\n",
    "print(\"-------------||-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Defining Label Matrix for Training, Testing, Validation\n",
      "TRAIN_Y Shape: torch.Size([2168])\n",
      "tensor([3, 4, 4,  ..., 6, 5, 2])\n",
      "TEST_Y Shape: torch.Size([270])\n",
      "tensor([1, 2, 1, 0, 0, 6, 6, 2, 3, 3, 5, 0, 0, 0, 0, 0, 5, 5, 0, 3, 5, 0, 6, 3,\n",
      "        6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 1, 6, 1, 0, 3, 3, 3, 3,\n",
      "        3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 3, 3, 0, 0, 0, 0, 5, 4, 4, 4, 4,\n",
      "        4, 3, 3, 3, 3, 3, 0, 3, 4, 4, 4, 1, 1, 3, 1, 1, 5, 1, 3, 4, 4, 4, 4, 4,\n",
      "        4, 4, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 3, 0, 6, 2, 0, 5, 3, 3, 5, 5, 5, 5,\n",
      "        5, 4, 4, 0, 4, 0, 4, 0, 3, 4, 4, 4, 1, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 0,\n",
      "        0, 2, 3, 3, 3, 3, 1, 1, 3, 0, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2, 4,\n",
      "        4, 4, 3, 3, 3, 4, 0, 3, 3, 3, 3, 0, 3, 3, 4, 4, 4, 4, 4, 4, 0, 4, 3, 2,\n",
      "        0, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 2, 3, 5, 5, 4, 1, 4, 4, 4, 3, 4, 4,\n",
      "        0, 4, 4, 4, 5, 2, 2, 2, 2, 4, 6, 6, 6, 6, 3, 4, 4, 4, 1, 3, 0, 3, 3, 5,\n",
      "        0, 2, 3, 3, 3, 3, 3, 2, 4, 4, 0, 0, 3, 2, 6, 6, 0, 3, 3, 3, 5, 1, 3, 4,\n",
      "        4, 2, 4, 4, 4, 3])\n",
      "VAL_Y Shape: torch.Size([270])\n",
      "tensor([3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 6, 6, 5, 6, 6, 3, 2, 6,\n",
      "        3, 4, 4, 4, 2, 6, 6, 0, 0, 3, 0, 4, 4, 3, 2, 3, 1, 6, 6, 5, 3, 4, 3, 5,\n",
      "        3, 1, 1, 3, 4, 5, 2, 3, 3, 3, 4, 5, 4, 0, 3, 3, 0, 2, 1, 1, 5, 2, 3, 3,\n",
      "        5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 2, 2, 4, 2, 4, 5, 5, 3, 2, 3, 1, 0,\n",
      "        3, 3, 4, 5, 4, 3, 3, 3, 3, 3, 0, 1, 2, 4, 4, 4, 3, 3, 3, 5, 2, 3, 2, 2,\n",
      "        2, 3, 2, 2, 0, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 0, 3, 0, 2,\n",
      "        3, 4, 1, 2, 5, 4, 3, 3, 3, 1, 5, 3, 4, 3, 2, 2, 1, 3, 3, 3, 3, 3, 6, 3,\n",
      "        3, 3, 6, 3, 3, 3, 2, 3, 2, 4, 2, 4, 2, 2, 1, 5, 6, 4, 3, 3, 3, 2, 5, 3,\n",
      "        3, 4, 3, 3, 3, 3, 3, 4, 6, 0, 3, 2, 2, 2, 5, 4, 4, 4, 4, 6, 3, 2, 2, 0,\n",
      "        2, 2, 2, 2, 2, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 3, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 6, 2, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3,\n",
      "        0, 3, 3, 3, 3, 3])\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Defining Label Matrix for Training, Testing, Validation\")\n",
    "TRAIN_Y = dataset.y[:train_global_num]\n",
    "print(\"TRAIN_Y Shape:\",TRAIN_Y.shape)\n",
    "print(TRAIN_Y)\n",
    "TEST_Y = dataset.y[train_global_num:train_global_num+test_global_num]\n",
    "print(\"TEST_Y Shape:\",TEST_Y.shape)\n",
    "print(TEST_Y)\n",
    "VAL_Y = dataset.y[train_global_num+test_global_num:train_global_num+test_global_num+val_global_num]\n",
    "print(\"VAL_Y Shape:\",VAL_Y.shape)\n",
    "print(VAL_Y)\n",
    "print(\"-------------||-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Defining P,K,n for Training, Testing, Validation\n",
      "0 \t TRAIN \t TEST \t VAL\n",
      "P \t 2168 \t 270 \t 270\n",
      "K \t 216 \t 27 \t 27\n",
      "n \t 1433 \t 1433 \t 1433\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Defining P,K,n for Training, Testing, Validation\")\n",
    "TRAIN_P = TRAIN_X.shape[0]\n",
    "TEST_P = TEST_X.shape[0]\n",
    "VAL_P = VAL_X.shape[0]\n",
    "TRAIN_K = int(TRAIN_P*r_global)\n",
    "TEST_K  = int(TEST_P*r_global)\n",
    "VAL_K = int(VAL_P*r_global)\n",
    "TRAIN_n = TRAIN_X.shape[1]\n",
    "TEST_n = TEST_X.shape[1]\n",
    "VAL_n = VAL_X.shape[1]\n",
    "print(\"0\",\"\\t\",\"TRAIN\",\"\\t\",\"TEST\",\"\\t\",\"VAL\")\n",
    "print(\"P\",\"\\t\",TRAIN_P,\"\\t\",TEST_P,\"\\t\",VAL_P)\n",
    "print(\"K\",\"\\t\",TRAIN_K,\"\\t\",TEST_K,\"\\t\",VAL_K)\n",
    "print(\"n\",\"\\t\",TRAIN_n,\"\\t\",TEST_n,\"\\t\",VAL_n)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Generating CustomDistributions for Training, Testing, Validation\n",
      "Generating for Training\n",
      "Generating for Testing\n",
      "Generating for Validation\n",
      "-------------||-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Generating CustomDistributions for Training, Testing, Validation\")\n",
    "class CustomDistribution(rv_continuous):\n",
    "    def _rvs(self,  size=None, random_state=None):\n",
    "        return random_state.standard_normal(size)\n",
    "print(\"Generating for Training\")\n",
    "TRAIN_temp = CustomDistribution(seed=1)\n",
    "TRAIN_temp2 = TRAIN_temp()  # get a frozen version of the distribution\n",
    "TRAIN_X_tilde = random(TRAIN_K, TRAIN_n, density=0.25, random_state=1, data_rvs=TRAIN_temp2.rvs)\n",
    "TRAIN_C = random(TRAIN_P, TRAIN_K, density=0.25, random_state=1, data_rvs=TRAIN_temp2.rvs)\n",
    "print(\"Generating for Testing\")\n",
    "TEST_temp = CustomDistribution(seed=1)\n",
    "TEST_temp2 = TEST_temp()  # get a frozen version of the distribution\n",
    "TEST_X_tilde = random(TEST_K, TEST_n, density=0.25, random_state=1, data_rvs=TEST_temp2.rvs)\n",
    "TEST_C = random(TEST_P, TEST_K, density=0.25, random_state=1, data_rvs=TEST_temp2.rvs)\n",
    "print(\"Generating for Validation\")\n",
    "VAL_temp = CustomDistribution(seed=1)\n",
    "VAL_temp2 = TEST_temp()  # get a frozen version of the distribution\n",
    "VAL_X_tilde = random(VAL_K, VAL_n, density=0.25, random_state=1, data_rvs=VAL_temp2.rvs)\n",
    "VAL_C = random(VAL_P, VAL_K, density=0.25, random_state=1, data_rvs=VAL_temp2.rvs)\n",
    "print(\"-------------||-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Additional Variabled Required: Labels\n",
      "TRAIN_Labels\n",
      " [3 4 4 ... 6 5 2]\n",
      "TEST_Labels\n",
      " [1 2 1 0 0 6 6 2 3 3 5 0 0 0 0 0 5 5 0 3 5 0 6 3 6 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 3 3 3 1 6 1 0 3 3 3 3 3 6 1 0 2 2 4 4 4 4 4 5 6 3 3 0 0 0 0 5 4 4 4 4 4 3\n",
      " 3 3 3 3 0 3 4 4 4 1 1 3 1 1 5 1 3 4 4 4 4 4 4 4 0 0 0 5 5 5 5 5 0 5 3 0 6\n",
      " 2 0 5 3 3 5 5 5 5 5 4 4 0 4 0 4 0 3 4 4 4 1 3 3 3 3 3 4 2 3 3 3 0 0 2 3 3\n",
      " 3 3 1 1 3 0 1 4 1 1 1 1 1 1 0 1 0 0 2 4 4 4 3 3 3 4 0 3 3 3 3 0 3 3 4 4 4\n",
      " 4 4 4 0 4 3 2 0 3 4 5 0 2 2 3 3 3 3 3 2 3 5 5 4 1 4 4 4 3 4 4 0 4 4 4 5 2\n",
      " 2 2 2 4 6 6 6 6 3 4 4 4 1 3 0 3 3 5 0 2 3 3 3 3 3 2 4 4 0 0 3 2 6 6 0 3 3\n",
      " 3 5 1 3 4 4 2 4 4 4 3]\n",
      "VAL_Labels\n",
      " [3 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 6 6 5 6 6 3 2 6 3 4 4 4 2 6 6 0 0 3 0 4 4\n",
      " 3 2 3 1 6 6 5 3 4 3 5 3 1 1 3 4 5 2 3 3 3 4 5 4 0 3 3 0 2 1 1 5 2 3 3 5 0\n",
      " 2 3 2 2 5 5 4 3 4 3 2 2 4 2 4 5 5 3 2 3 1 0 3 3 4 5 4 3 3 3 3 3 0 1 2 4 4\n",
      " 4 3 3 3 5 2 3 2 2 2 3 2 2 0 4 4 3 3 3 3 3 3 3 3 3 3 0 0 3 0 3 0 2 3 4 1 2\n",
      " 5 4 3 3 3 1 5 3 4 3 2 2 1 3 3 3 3 3 6 3 3 3 6 3 3 3 2 3 2 4 2 4 2 2 1 5 6\n",
      " 4 3 3 3 2 5 3 3 4 3 3 3 3 3 4 6 0 3 2 2 2 5 4 4 4 4 6 3 2 2 0 2 2 2 2 2 3\n",
      " 4 4 4 3 3 4 4 3 3 3 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 2 3 3 3 2 6 2 3 3 4 4 3\n",
      " 3 3 3 3 3 0 3 3 3 3 3]\n",
      "\n",
      "Additional Variabled Required: Features\n",
      "TRAIN_features\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "TEST_features\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "VAL_features\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Additional Variabled Required: Labels\")\n",
    "TRAIN_labels = TRAIN_Y\n",
    "TRAIN_labels = TRAIN_labels.numpy()\n",
    "print(\"TRAIN_Labels\\n\",TRAIN_labels)\n",
    "TEST_labels = TEST_Y\n",
    "TEST_labels = TEST_labels.numpy()\n",
    "print(\"TEST_Labels\\n\",TEST_labels)\n",
    "VAL_labels = VAL_Y\n",
    "VAL_labels = VAL_labels.numpy()\n",
    "print(\"VAL_Labels\\n\",VAL_labels)\n",
    "print()\n",
    "print(\"Additional Variabled Required: Features\")\n",
    "TRAIN_features = TRAIN_X.numpy()\n",
    "TEST_features = TEST_X.numpy()\n",
    "VAL_features = VAL_X.numpy()\n",
    "print(\"TRAIN_features\\n\",TRAIN_features)\n",
    "print(\"TEST_features\\n\",TEST_features)\n",
    "print(\"VAL_features\\n\",VAL_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertScipyToTensor(coo):\n",
    "    try:\n",
    "        coo = coo.tocoo()\n",
    "    except:\n",
    "        coo = coo\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha_param,beta_param,gamma_param,lambda_param,delta_param,C,X_tilde,theta,X,Y):\n",
    "    p = X.shape[0]\n",
    "    k = int(p*r_global)\n",
    "    n = X.shape[1]\n",
    "    ones = csr_matrix(np.ones((k,k)))\n",
    "    ones = convertScipyToTensor(ones)\n",
    "    ones = ones.to_dense()\n",
    "    J = np.outer(np.ones(k), np.ones(k))/k\n",
    "    J = csr_matrix(J)\n",
    "    J = convertScipyToTensor(J)\n",
    "    J = J.to_dense()\n",
    "    zeros = csr_matrix(np.zeros((p,k)))\n",
    "    zeros = convertScipyToTensor(zeros)\n",
    "    zeros = zeros.to_dense()\n",
    "    X_tilde = convertScipyToTensor(X_tilde)\n",
    "    X_tilde = X_tilde.to_dense()\n",
    "    C = convertScipyToTensor(C)\n",
    "    C = C.to_dense()\n",
    "    eye = torch.eye(k)\n",
    "    try:\n",
    "        theta = convertScipyToTensor(theta)\n",
    "    except:\n",
    "        theta = theta\n",
    "    try:\n",
    "        X = convertScipyToTensor(X)\n",
    "        X = X.to_dense()\n",
    "    except:\n",
    "        X = X\n",
    "\n",
    "    def one_hot(x, class_count):\n",
    "        return torch.eye(class_count)[x, :]\n",
    "\n",
    "    P = Y.numpy()\n",
    "    NO_OF_CLASSES = len(set(Y))\n",
    "    P = one_hot(P,NO_OF_CLASSES)\n",
    "    \n",
    "    if(torch.cuda.is_available()):\n",
    "        # print(\"yes\")\n",
    "        X_tilde = X_tilde.cuda()\n",
    "        C = C.cuda()\n",
    "        theta = theta.cuda()\n",
    "        X = X.cuda()\n",
    "        J = J.cuda()\n",
    "        P = P.cuda()\n",
    "        zeros = zeros.cuda()\n",
    "        ones = ones.cuda()\n",
    "        eye = eye.cuda()\n",
    "    def update(X_tilde,C,i):\n",
    "        global L\n",
    "        thetaC = theta@C\n",
    "        CT = torch.transpose(C,0,1)\n",
    "        X_tildeT = torch.transpose(X_tilde,0,1)\n",
    "        CX_tilde = C@X_tilde\n",
    "        t1 = CT@thetaC + J\n",
    "        term_bracket = torch.linalg.pinv(t1)\n",
    "        thetacX_tilde = thetaC@(X_tilde)\n",
    "        \n",
    "        L = 1/k\n",
    "\n",
    "        t1 = -2*gamma_param*(thetaC@term_bracket)\n",
    "        t2 = alpha_param*(CX_tilde-X)@(X_tildeT)\n",
    "        t3 = 2*thetacX_tilde@(X_tildeT)\n",
    "        t4 = lambda_param*(C@ones)\n",
    "        t5 = 2*beta_param*(thetaC@CT@thetaC)\n",
    "        t6 = delta_param*P@torch.transpose((CT@P),0,1)\n",
    "        T2 = (t1+t2+t3+t4+t5+t6)/L\n",
    "        Cnew = (C-T2).maximum(zeros)\n",
    "        t1 = CT@thetaC*(2/alpha_param)\n",
    "        t2 = CT@C\n",
    "        t1 = torch.linalg.pinv(t1+t2)\n",
    "        t1 = t1@CT\n",
    "        t1 = t1@X\n",
    "        X_tilde_new = t1\n",
    "        Cnew[Cnew<thresh] = thresh\n",
    "        for i in range(len(Cnew)):\n",
    "            Cnew[i] = Cnew[i]/torch.linalg.norm(Cnew[i],1)\n",
    "        for i in range(len(X_tilde_new)):\n",
    "            X_tilde_new[i] = X_tilde_new[i]/torch.linalg.norm(X_tilde_new[i],1)\n",
    "        return X_tilde_new,Cnew\n",
    "\n",
    "\n",
    "    for i in tqdm(range(20)):\n",
    "        X_tilde,C = update(X_tilde,C,i)\n",
    "\n",
    "    return X_tilde,C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n",
      "Generating X_t_0,C_0\n",
      "-------------|TRAINING|-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------|TESTING|-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 64.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------|VALIDATION|-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 69.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------||-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------||-------------\")\n",
    "print(\"Generating X_t_0,C_0\")\n",
    "print(\"-------------|TRAINING|-------------\")\n",
    "TRAIN_X_t_0,TRAIN_C_0 = experiment(alpha_param,beta_param,gamma_param,lambda_param,delta_param,TRAIN_C,TRAIN_X_tilde,TRAIN_theta,TRAIN_X,TRAIN_Y)\n",
    "TRAIN_L = TRAIN_theta\n",
    "TRAIN_C_0 = TRAIN_C_0.cpu().detach().numpy()\n",
    "TRAIN_X_t_0 = TRAIN_X_t_0.cpu().detach().numpy()\n",
    "TRAIN_C_t_0 = TRAIN_C_0.T\n",
    "try:\n",
    "    TRAIN_L = TRAIN_L.cpu().detach().numpy()\n",
    "except:\n",
    "    TRAIN_L = TRAIN_L\n",
    "print(\"-------------|TESTING|-------------\")\n",
    "TEST_X_t_0,TEST_C_0 = experiment(alpha_param,beta_param,gamma_param,lambda_param,delta_param,TEST_C,TEST_X_tilde,TEST_theta,TEST_X,TEST_Y)\n",
    "TEST_L = TEST_theta\n",
    "TEST_C_0 = TEST_C_0.cpu().detach().numpy()\n",
    "TEST_X_t_0 = TEST_X_t_0.cpu().detach().numpy()\n",
    "TEST_C_t_0 = TEST_C_0.T\n",
    "try:\n",
    "    TEST_L = TEST_L.cpu().detach().numpy()\n",
    "except:\n",
    "    TEST_L = TEST_L\n",
    "print(\"-------------|VALIDATION|-------------\")\n",
    "VAL_X_t_0,VAL_C_0 = experiment(alpha_param,beta_param,gamma_param,lambda_param,delta_param,VAL_C,VAL_X_tilde,VAL_theta,VAL_X,VAL_Y)\n",
    "VAL_L = VAL_theta\n",
    "VAL_C_0 = VAL_C_0.cpu().detach().numpy()\n",
    "VAL_X_t_0 = VAL_X_t_0.cpu().detach().numpy()\n",
    "VAL_C_t_0 = VAL_C_0.T\n",
    "try:\n",
    "    VAL_L = VAL_L.cpu().detach().numpy()\n",
    "except:\n",
    "    VAL_L = VAL_L\n",
    "\n",
    "print(\"-------------||-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(TRAIN_X.shape[1], 64)\n",
    "        self.conv2 = GCNConv(64, NO_OF_CLASSES)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, TRAIN_x, edge_index):\n",
    "\n",
    "        #print(\"Checking 1: x\", x.shape, \"Edge index:\", edge_index.shape)\n",
    "        TRAIN_x = self.conv1(TRAIN_x, edge_index)\n",
    "        #print(\"Checking 2: convolution done, new x:\", x.shape)\n",
    "        TRAIN_x = F.relu(TRAIN_x)\n",
    "        #print(\"Checking 3: x\", x.shape, \"training:\", self.training)\n",
    "        TRAIN_x = F.dropout(TRAIN_x, training=self.training)\n",
    "        #print(\"Checking 4: dropout done new x\", x.shape, \"Edge index:\", edge_index.shape)\n",
    "        TRAIN_x = self.conv2(TRAIN_x, edge_index)\n",
    "        #print(\"Checking 5: x\", x.shape)\n",
    "\n",
    "        return F.log_softmax(TRAIN_x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    for i in [1,2,3,4,5,6,7,8,9,10]: \n",
    "        t = []\n",
    "        TRAIN_C_0_new=np.zeros(TRAIN_C_0.shape)\n",
    "        for i in range(TRAIN_C_0.shape[0]):\n",
    "            TRAIN_C_0_new[i][np.argmax(TRAIN_C_0[i])]=1\n",
    "        from scipy import sparse\n",
    "        TRAIN_Lc=TRAIN_C_0_new.T@TRAIN_L@TRAIN_C_0_new\n",
    "        TRAIN_Wc=(-1*TRAIN_Lc)*(1-np.eye(TRAIN_Lc.shape[0]))\n",
    "        TRAIN_Wc[TRAIN_Wc<0.1]=0\n",
    "        TRAIN_Wc=sparse.csr_matrix(TRAIN_Wc)\n",
    "        TRAIN_Wc = TRAIN_Wc.tocoo()\n",
    "        TRAIN_row = torch.from_numpy(TRAIN_Wc.row).to(torch.long)\n",
    "        TRAIN_col = torch.from_numpy(TRAIN_Wc.col).to(torch.long)\n",
    "\n",
    "        TRAIN_edge_index_coarsen2 = torch.stack([TRAIN_row, TRAIN_col], dim=0)\n",
    "        TRAIN_edge_weight = torch.from_numpy(TRAIN_Wc.data)\n",
    "        def one_hot(x, class_count):\n",
    "            return torch.eye(class_count)[x, :]\n",
    "\n",
    "        device = torch.device('cpu')\n",
    "        TRAIN_Y = TRAIN_labels\n",
    "        TRAIN_Y = one_hot(TRAIN_Y,NO_OF_CLASSES)\n",
    "        TRAIN_P=np.linalg.pinv(TRAIN_C_0_new)\n",
    "        TRAIN_labels_coarse = torch.argmax(torch.sparse.mm(torch.Tensor(TRAIN_P).double() , TRAIN_Y.double()).double() , 1)\n",
    "\n",
    "\n",
    "        TRAIN_Wc=TRAIN_Wc.toarray()\n",
    "        TRAIN_adjtemp = torch.tensor(TRAIN_Wc)\n",
    "        TRAIN_edge_list_temp = dense_to_sparse(TRAIN_adjtemp)[0]\n",
    "        TRAIN_number_of_edges = TRAIN_edge_list_temp.shape[1]\n",
    "        n = TRAIN_labels_coarse.shape[0]\n",
    "        TRAIN_sparsity = 2*TRAIN_number_of_edges/(n*(n-1))\n",
    "\n",
    "        TRAIN_C2=np.linalg.pinv(TRAIN_C_0_new)\n",
    "        model=Net().to(device)\n",
    "        device = torch.device('cpu')\n",
    "        lr=0.01\n",
    "        decay=0.0001\n",
    "        try:\n",
    "            TRAIN_X=np.array(TRAIN_features.todense())\n",
    "        except:\n",
    "            TRAIN_X = np.array(TRAIN_features)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "        TRAIN_x=sample(range(0, int(TRAIN_K)), TRAIN_K)\n",
    "        TRAIN_Xt=TRAIN_P@TRAIN_X\n",
    "        def train():\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(torch.Tensor(TRAIN_Xt).to(device),TRAIN_edge_index_coarsen2)\n",
    "            loss = F.nll_loss(out[TRAIN_x], TRAIN_labels_coarse[TRAIN_x])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            return loss\n",
    "        def test():\n",
    "            model.eval()\n",
    "            out = model(TEST_X, TEST_edge_index)\n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            NO_OF_NODES = test_global_num\n",
    "            zz=sample(range(0, int(test_global_num)), NO_OF_NODES)\n",
    "            test_correct = pred[zz] == TEST_Y[zz]  # Check against ground-truth labels.\n",
    "            test_acc = int(test_correct.sum()) / NO_OF_NODES # Derive ratio of correct predictions.\n",
    "            return test_acc\n",
    "        def val():\n",
    "            model.eval()\n",
    "            out = model(VAL_X, VAL_edge_index)\n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            NO_OF_NODES = VAL_X.shape[0]\n",
    "            zz=sample(range(0,int(val_global_num)), val_global_num)\n",
    "            test_correct = pred[zz] == TEST_Y[zz]  # Check against ground-truth labels.\n",
    "            val_acc = int(test_correct.sum()) / val_global_num # Derive ratio of correct predictions.\n",
    "            return val_acc\n",
    "\n",
    "        now1 = datetime.now()\n",
    "        losses=[]\n",
    "        for epoch in range(200):\n",
    "            loss=train()\n",
    "            losses.append(loss)\n",
    "\n",
    "        now2 = datetime.now()        \n",
    "        TRAIN_pred=model(torch.Tensor(TRAIN_Xt).to(device),TRAIN_edge_index_coarsen2).argmax(dim=1)        \n",
    "        def train_accuracy():\n",
    "            model.eval()\n",
    "            correct = (TRAIN_pred[TRAIN_x] == TRAIN_labels_coarse[TRAIN_x]).sum()\n",
    "            acc = int(correct) /len(TRAIN_x)\n",
    "            return acc\n",
    "        t+=[(now2-now1).total_seconds()]\n",
    "        NO_OF_NODES = TRAIN_X.shape[0]\n",
    "        zz=sample(range(0, int(NO_OF_NODES)), NO_OF_NODES)\n",
    "        TRAIN_Wc=sparse.csr_matrix(TRAIN_adj)\n",
    "        TRAIN_Wc = TRAIN_Wc.tocoo()\n",
    "        TRAIN_row = torch.from_numpy(TRAIN_Wc.row).to(torch.long)\n",
    "        TRAIN_col = torch.from_numpy(TRAIN_Wc.col).to(torch.long)\n",
    "        TRAIN_edge_index_coarsen = torch.stack([TRAIN_row, TRAIN_col], dim=0)\n",
    "        TRAIN_edge_weight = torch.from_numpy(TRAIN_Wc.data)\n",
    "        pred=model(torch.Tensor(TRAIN_X),TRAIN_edge_index_coarsen).argmax(dim=1)\n",
    "        pred=np.array(pred)\n",
    "        correct =(pred[zz]==TRAIN_labels[zz]).sum()\n",
    "        acc = int(correct) /NO_OF_NODES\n",
    "        test_acc = test()\n",
    "        val_acc = val()\n",
    "        print(f'Train Accuracy: {acc:.4f}')\n",
    "        print(f'Test Accuracy: {test_acc:.4f}')\n",
    "        print(f'Validtion Accuracy: {val_acc:.4f}')\n",
    "        return acc\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7306\n",
      "Test Accuracy: 0.5852\n",
      "Validtion Accuracy: 0.1519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7306273062730627"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_NODES = TEST_X.shape[0]\n",
    "START = train_global_num\n",
    "zz=sample(range(START, START + int(test_global_num)), NO_OF_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_global_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([270, 1433])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
